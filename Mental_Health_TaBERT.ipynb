{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5N9F-c9VbR7Q"
      },
      "outputs": [],
      "source": [
        "# =============================================\n",
        "# Academic Performance Pipeline (Tabular + XAI)\n",
        "# =============================================\n",
        "# Requirements (install as needed):\n",
        "# pip install pandas numpy scikit-learn scipy torch torchvision torchaudio shap lime transformers einops\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from collections import defaultdict\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "# Optional libraries\n",
        "try:\n",
        "    import shap\n",
        "    HAVE_SHAP = True\n",
        "except Exception:\n",
        "    HAVE_SHAP = False\n",
        "\n",
        "try:\n",
        "    from lime.lime_tabular import LimeTabularExplainer\n",
        "    HAVE_LIME = True\n",
        "except Exception:\n",
        "    HAVE_LIME = False\n",
        "\n",
        "# Deep/transformer libs\n",
        "try:\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    from torch.utils.data import Dataset, DataLoader\n",
        "    from einops import rearrange\n",
        "    HAVE_TORCH = True\n",
        "except Exception:\n",
        "    HAVE_TORCH = False\n",
        "\n",
        "try:\n",
        "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "    HAVE_TRANSFORMERS = True\n",
        "except Exception:\n",
        "    HAVE_TRANSFORMERS = False\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 0) CONFIG: paths & columns\n",
        "# -----------------------------\n",
        "RNG = 42\n",
        "DATA_PATH = \"your_dataset.csv\"  # <-- set your CSV path here\n",
        "\n",
        "TARGET_COL = \"CGPA_label\"  # Binary or multi-class label for academic performance\n",
        "TEXT_COL = None            # e.g., \"essay\" if you have a text field; otherwise leave None\n",
        "\n",
        "# Feature groups (edit to match your data)\n",
        "MENTAL_FEATURES = [\n",
        "    \"Financial_Stress\",\"Anxiety_Score\",\"Depression_Score\",\"Stress_Level\",\n",
        "    \"Sleep_Quality\",\"Counseling_Service_Use\",\"Substance_Use\",\"Chronic_Illness\"\n",
        "]\n",
        "PERSONAL_FEATURES = [\"Age\",\"Relationship_Status\",\"Extracurricular_Involvement\",\"Family_History\",\"Gender\"]\n",
        "SOCIAL_FEATURES = [\"Residence_Type\",\"Physical_Activity\",\"Social_Support\",\"Diet_Quality\"]\n",
        "COURSE_COL = \"Course\"     # for subgroup analysis\n",
        "NUMERIC_FEATURES = [\"Age\",\"CGPA\",\"Stress_Level\",\"Depression_Score\",\"Anxiety_Score\",\n",
        "                    \"Financial_Stress\",\"Semester_Credit_Load\"]\n",
        "CATEGORICAL_FEATURES = [\n",
        "    \"Course\",\"Gender\",\"Sleep_Quality\",\"Physical_Activity\",\"Diet_Quality\",\"Social_Support\",\n",
        "    \"Relationship_Status\",\"Substance_Use\",\"Counseling_Service_Use\",\"Family_History\",\n",
        "    \"Chronic_Illness\",\"Extracurricular_Involvement\",\"Residence_Type\"\n",
        "]\n",
        "\n",
        "# Helper: safe intersection with dataframe columns\n",
        "def keep_existing(cols: List[str], df: pd.DataFrame) -> List[str]:\n",
        "    return [c for c in cols if c in df.columns]\n",
        "\n",
        "# -----------------------------------------\n",
        "# 1) DATA LOAD (expects a CSV)\n",
        "# -----------------------------------------\n",
        "def load_data(path: str) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)\n",
        "    return df\n",
        "\n",
        "# -----------------------------------------\n",
        "# 2) PREPROCESS (impute/scale/encode)\n",
        "# 2.1) feature categorization (already via lists)\n",
        "# -----------------------------------------\n",
        "def make_preprocessor(df: pd.DataFrame):\n",
        "    num_cols = keep_existing(NUMERIC_FEATURES, df)\n",
        "    cat_cols = keep_existing(CATEGORICAL_FEATURES, df)\n",
        "\n",
        "    numeric = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler())\n",
        "    ])\n",
        "    categorical = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "    ])\n",
        "    pre = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", numeric, num_cols),\n",
        "            (\"cat\", categorical, cat_cols)\n",
        "        ],\n",
        "        remainder=\"drop\"\n",
        "    )\n",
        "    return pre, num_cols, cat_cols\n",
        "\n",
        "# -----------------------------------------\n",
        "# 3) FEATURE SELECTION\n",
        "#    IG, Gain Ratio, Entropy, Gini\n",
        "# -----------------------------------------\n",
        "def entropy_of_series(x: pd.Series) -> float:\n",
        "    vals, counts = np.unique(x.astype(str), return_counts=True)\n",
        "    p = counts / counts.sum()\n",
        "    return -(p * np.log2(p + 1e-12)).sum()\n",
        "\n",
        "def gini_of_series(x: pd.Series) -> float:\n",
        "    vals, counts = np.unique(x.astype(str), return_counts=True)\n",
        "    p = counts / counts.sum()\n",
        "    return 1.0 - (p**2).sum()\n",
        "\n",
        "def info_gain(feature: pd.Series, target: pd.Series) -> float:\n",
        "    h_y = entropy_of_series(target)\n",
        "    # conditional entropy H(Y|X)\n",
        "    cond_entropy = 0.0\n",
        "    for v, cnt in target.groupby(feature.astype(str)).size().items():\n",
        "        idx = (feature.astype(str) == v)\n",
        "        h = entropy_of_series(target[idx])\n",
        "        cond_entropy += (idx.mean()) * h\n",
        "    return h_y - cond_entropy\n",
        "\n",
        "def gain_ratio(feature: pd.Series, target: pd.Series) -> float:\n",
        "    ig = info_gain(feature, target)\n",
        "    split_info = entropy_of_series(feature)\n",
        "    return ig / (split_info + 1e-12)\n",
        "\n",
        "def feature_scores(df: pd.DataFrame, y: pd.Series, feature_list: List[str]) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for f in feature_list:\n",
        "        s = df[f]\n",
        "        # convert numeric/cat to string categories for entropy/gini stability\n",
        "        ig = info_gain(s, y)\n",
        "        gr = gain_ratio(s, y)\n",
        "        ent = entropy_of_series(s)\n",
        "        gi = gini_of_series(s)\n",
        "        rows.append({\"feature\": f, \"info_gain\": ig, \"gain_ratio\": gr, \"entropy\": ent, \"gini\": gi})\n",
        "    return pd.DataFrame(rows).sort_values(\"info_gain\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "def select_by_criterion(scores: pd.DataFrame, criterion: str, top_k: Optional[int]=None, thr: Optional[float]=None) -> List[str]:\n",
        "    assert criterion in {\"info_gain\",\"gain_ratio\",\"entropy\",\"gini\"}\n",
        "    if criterion in {\"entropy\",\"gini\"}:\n",
        "        # lower is simpler; but user asked to *apply* “Entropy” as selection:\n",
        "        # we choose top_k by descending info_gain but report entropy; or you can pick lowest entropy\n",
        "        sort_asc = True\n",
        "    else:\n",
        "        sort_asc = False\n",
        "    s = scores.sort_values(criterion, ascending=sort_asc)\n",
        "    if thr is not None:\n",
        "        if sort_asc:\n",
        "            chosen = s[s[criterion] <= thr]\n",
        "        else:\n",
        "            chosen = s[s[criterion] >= thr]\n",
        "    else:\n",
        "        chosen = s\n",
        "    if top_k is not None:\n",
        "        chosen = chosen.head(top_k)\n",
        "    return chosen[\"feature\"].tolist()\n",
        "\n",
        "# -----------------------------------------\n",
        "# 4) MODELS\n",
        "# -----------------------------------------\n",
        "def classical_models(random_state=RNG) -> Dict[str, object]:\n",
        "    return {\n",
        "        \"SVM\": SVC(kernel=\"rbf\", probability=True, random_state=random_state),\n",
        "        \"LR\": LogisticRegression(max_iter=2000, n_jobs=None),\n",
        "        \"RF\": RandomForestClassifier(n_estimators=300, max_depth=None, random_state=random_state),\n",
        "        \"GB\": GradientBoostingClassifier(random_state=random_state),\n",
        "    }\n",
        "\n",
        "# Optional LSTM for tabular (treats rows as sequences of features)\n",
        "class TabularLSTM(nn.Module):\n",
        "    def __init__(self, n_features, hidden=64, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=hidden, num_layers=1, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden*n_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, F) -> (B, F, 1)\n",
        "        x = x.unsqueeze(-1)\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        return self.fc(out)\n",
        "\n",
        "# Minimal TabTransformer (TaBERT-style) for tabular\n",
        "class TabDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
        "\n",
        "class TabTransformer(nn.Module):\n",
        "    def __init__(self, n_features, d_model=64, n_heads=4, depth=2, num_classes=2, p=0.1):\n",
        "        super().__init__()\n",
        "        self.in_proj = nn.Linear(n_features, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, batch_first=True, dropout=p)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
        "        self.cls = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, F) -> treat as a length-F sequence of 1 token dimension\n",
        "        x = self.in_proj(x)              # (B, d_model)\n",
        "        x = x.unsqueeze(1).repeat(1, 1, 1)  # single token\n",
        "        h = self.encoder(x)              # (B, 1, d_model)\n",
        "        return self.cls(h[:,0,:])\n",
        "\n",
        "def train_torch_classifier(model, X_train, y_train, X_val, y_val, epochs=20, lr=1e-3, batch=64):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    ds_tr = TabDataset(X_train, y_train)\n",
        "    ds_va = TabDataset(X_val, y_val)\n",
        "    dl_tr = DataLoader(ds_tr, batch_size=batch, shuffle=True)\n",
        "    dl_va = DataLoader(ds_va, batch_size=batch, shuffle=False)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        model.train()\n",
        "        for xb, yb in dl_tr:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = loss_fn(logits, yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "    # predict on val\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = []\n",
        "        for xb, _ in dl_va:\n",
        "            xb = xb.to(device)\n",
        "            logits = model(xb)\n",
        "            preds.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "    y_pred = np.concatenate(preds)\n",
        "    return y_pred, model\n",
        "\n",
        "# Optional text transformers (BERT/RoBERTa) if a text column exists\n",
        "def infer_text_transformer(model_name, texts, labels):\n",
        "    # Placeholder minimal inference; for full training, fine-tune as needed.\n",
        "    return None  # kept simple to avoid heavy training here\n",
        "\n",
        "\n",
        "# -----------------------------------------\n",
        "# 5) Evaluation helpers\n",
        "# -----------------------------------------\n",
        "def score_pack(y_true, y_pred) -> Dict[str, float]:\n",
        "    return {\n",
        "        \"acc\": accuracy_score(y_true, y_pred),\n",
        "        \"precision\": precision_score(y_true, y_pred, average=\"weighted\", zero_division=0),\n",
        "        \"recall\": recall_score(y_true, y_pred, average=\"weighted\", zero_division=0),\n",
        "        \"f1\": f1_score(y_true, y_pred, average=\"weighted\", zero_division=0),\n",
        "    }\n",
        "\n",
        "def run_classical(X, y, models: Dict[str, object]):\n",
        "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=RNG, stratify=y)\n",
        "    results = {}\n",
        "    for name, mdl in models.items():\n",
        "        mdl.fit(X_tr, y_tr)\n",
        "        y_pred = mdl.predict(X_te)\n",
        "        results[name] = score_pack(y_te, y_pred)\n",
        "    return results\n",
        "\n",
        "def run_torch_model(X, y, model_ctor, **kwargs):\n",
        "    if not HAVE_TORCH:\n",
        "        return {}\n",
        "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=RNG, stratify=y)\n",
        "    n_features = X.shape[1]\n",
        "    model = model_ctor(n_features=n_features, num_classes=len(np.unique(y)), **kwargs)\n",
        "    y_pred, trained = train_torch_classifier(model, X_tr, y_tr, X_te, y_te, epochs=kwargs.get(\"epochs\", 20))\n",
        "    return {\"TorchModel\": score_pack(y_te, y_pred)}, trained\n",
        "\n",
        "# -----------------------------------------\n",
        "# 5.x) Category × Selector × Models\n",
        "# -----------------------------------------\n",
        "def prepare_matrix(df, target_col, feature_list, preprocessor):\n",
        "    X = df[feature_list].copy()\n",
        "    y = df[target_col].copy().astype(int)\n",
        "    X_proc = preprocessor.fit_transform(df[feature_list])\n",
        "    return X, y, X_proc\n",
        "\n",
        "def evaluate_block(df, target_col, group_features, selector_name, scores_df, pre_template, top_k=None):\n",
        "    selected = select_by_criterion(scores_df, selector_name, top_k=top_k)\n",
        "    if len(selected) == 0:\n",
        "        return {\"selected\": [], \"results\": {}}\n",
        "    pre = pre_template\n",
        "    X = df[selected]\n",
        "    y = df[target_col].astype(int)\n",
        "    X_proc = pre.fit_transform(X)\n",
        "\n",
        "    cls_res = run_classical(X_proc, y, classical_models())\n",
        "    torch_res = {}\n",
        "    if HAVE_TORCH:\n",
        "        # LSTM\n",
        "        res_lstm, _ = run_torch_model(X_proc, y, model_ctor=lambda n_features, num_classes, **kw: TabularLSTM(n_features, hidden=64, num_classes=num_classes), epochs=20)\n",
        "        # TabTransformer (TaBERT-style)\n",
        "        res_tab, _ = run_torch_model(X_proc, y, model_ctor=lambda n_features, num_classes, **kw: TabTransformer(n_features, d_model=64, n_heads=4, depth=2, num_classes=num_classes), epochs=20)\n",
        "        torch_res.update({ \"LSTM\": list(res_lstm.values())[0], \"TaBERT\": list(res_tab.values())[0] })\n",
        "    return {\"selected\": selected, \"results\": {**cls_res, **torch_res}}\n",
        "\n",
        "def run_all_blocks(df: pd.DataFrame, target_col: str):\n",
        "\n",
        "    # Preprocessor used inside each block (re-fit per block)\n",
        "    pre, _, _ = make_preprocessor(df)\n",
        "\n",
        "    blocks = []\n",
        "    for group_name, feats in [\n",
        "        (\"Mental\", keep_existing(MENTAL_FEATURES, df)),\n",
        "        (\"Social\", keep_existing(SOCIAL_FEATURES, df)),\n",
        "        (\"Personal\", keep_existing(PERSONAL_FEATURES, df)),\n",
        "        (\"CGPA_Target\", keep_existing(MENTAL_FEATURES+SOCIAL_FEATURES+PERSONAL_FEATURES, df))\n",
        "    ]:\n",
        "        if len(feats)==0:\n",
        "            continue\n",
        "        scores = feature_scores(df[feats], df[TARGET_COL], feats)\n",
        "        for sel in [\"info_gain\", \"gain_ratio\", \"entropy\"]:\n",
        "            block = evaluate_block(df, TARGET_COL, feats, sel, scores, pre, top_k=None)\n",
        "            blocks.append({\n",
        "                \"group\": group_name,\n",
        "                \"selector\": sel,\n",
        "                \"selected_features\": block[\"selected\"],\n",
        "                \"results\": block[\"results\"]\n",
        "            })\n",
        "    return blocks\n",
        "\n",
        "# -----------------------------------------\n",
        "# 6) Course-wise performance\n",
        "# -----------------------------------------\n",
        "def course_wise_performance(df: pd.DataFrame, target_col: str):\n",
        "    out = []\n",
        "    models = classical_models()\n",
        "    pre, _, _ = make_preprocessor(df)\n",
        "    for c, dsub in df.groupby(COURSE_COL):\n",
        "        if dsub[target_col].nunique() < 2:  # need both classes\n",
        "            continue\n",
        "        X = dsub.drop(columns=[target_col])\n",
        "        y = dsub[target_col].astype(int)\n",
        "        X_proc = pre.fit_transform(X)\n",
        "        res = run_classical(X_proc, y, models)\n",
        "        out.append({\"course\": c, **{f\"{m}_{k}\": v for m,sc in res.items() for k,v in sc.items()}})\n",
        "    return pd.DataFrame(out)\n",
        "\n",
        "# -----------------------------------------\n",
        "# 7) LIME & SHAP\n",
        "# -----------------------------------------\n",
        "def run_xai_examples(df: pd.DataFrame, target_col: str, feature_list: List[str], preprocessor):\n",
        "    X = df[feature_list]\n",
        "    y = df[target_col].astype(int)\n",
        "    X_proc = preprocessor.fit_transform(X)\n",
        "\n",
        "    rf = RandomForestClassifier(n_estimators=300, random_state=RNG).fit(X_proc, y)\n",
        "\n",
        "    xai = {}\n",
        "    if HAVE_SHAP:\n",
        "        explainer = shap.TreeExplainer(rf)\n",
        "        shap_values = explainer.shap_values(X_proc[:500])  # sample\n",
        "        xai[\"shap_values_shape\"] = [np.array(v).shape for v in shap_values] if isinstance(shap_values, list) else np.array(shap_values).shape\n",
        "\n",
        "    if HAVE_LIME:\n",
        "        explainer = LimeTabularExplainer(\n",
        "            training_data=np.array(X_proc),\n",
        "            mode=\"classification\",\n",
        "            feature_names=[f\"f{i}\" for i in range(X_proc.shape[1])],\n",
        "            discretize_continuous=False\n",
        "        )\n",
        "        explanation = explainer.explain_instance(\n",
        "            data_row=X_proc[0],\n",
        "            predict_fn=rf.predict_proba,\n",
        "            num_features=10\n",
        "        )\n",
        "        xai[\"lime_top\"] = explanation.as_list()\n",
        "\n",
        "    return xai\n",
        "\n",
        "# -----------------------------------------\n",
        "# 8) Statistical tests\n",
        "# -----------------------------------------\n",
        "def statistical_tests(df: pd.DataFrame, target_col: str, feature_list: List[str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    For numeric features: t-test / ANOVA across target classes.\n",
        "    For categorical features: chi-square (contingency) and z-test (two-proportion if binary).\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    y = df[target_col]\n",
        "    classes = sorted(y.unique())\n",
        "\n",
        "    for f in feature_list:\n",
        "        s = df[f]\n",
        "        if np.issubdtype(s.dtype, np.number):\n",
        "            groups = [s[y==cl] for cl in classes]\n",
        "            # t-test only if binary classes\n",
        "            p_t = stats.ttest_ind(groups[0], groups[1], equal_var=False, nan_policy=\"omit\").pvalue if len(groups)==2 else np.nan\n",
        "            # one-way ANOVA if >=2 classes\n",
        "            p_a = stats.f_oneway(*groups).pvalue if len(groups)>=2 else np.nan\n",
        "            p_chi = np.nan\n",
        "            p_z = np.nan\n",
        "        else:\n",
        "            # chi-square\n",
        "            tbl = pd.crosstab(s, y)\n",
        "            chi2_stat, p_chi, dof, _ = stats.chi2_contingency(tbl)\n",
        "            # z-test for two-proportion if binary y and binary feature\n",
        "            if len(classes)==2 and tbl.shape[0]==2:\n",
        "                # compute pooled proportion test\n",
        "                counts = tbl.values\n",
        "                p1 = counts[0,0] / counts[:,0].sum()\n",
        "                p2 = counts[0,1] / counts[:,1].sum()\n",
        "                p_pool = (counts[0,0] + counts[0,1]) / counts.sum()\n",
        "                se = np.sqrt(p_pool*(1-p_pool)*(1/counts[:,0].sum() + 1/counts[:,1].sum()))\n",
        "                z = (p1 - p2) / (se + 1e-12)\n",
        "                p_z = 2 * (1 - stats.norm.cdf(abs(z)))\n",
        "            else:\n",
        "                p_z = np.nan\n",
        "            p_t = np.nan\n",
        "            p_a = np.nan\n",
        "\n",
        "        rows.append({\"feature\": f, \"t_test_p\": p_t, \"anova_p\": p_a, \"chi2_p\": p_chi, \"ztest_p\": p_z})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# -----------------------------------------\n",
        "# MAIN\n",
        "# -----------------------------------------\n",
        "def main():\n",
        "    df = load_data(DATA_PATH)\n",
        "\n",
        "    # (Optional) create/derive target if CGPA numeric exists:\n",
        "    if TARGET_COL not in df.columns and \"CGPA\" in df.columns:\n",
        "        # Example: high (>=3.5) vs others\n",
        "        df[\"CGPA_label\"] = (df[\"CGPA\"] >= 3.5).astype(int)\n",
        "\n",
        "    assert TARGET_COL in df.columns, f\"Target column '{TARGET_COL}' missing.\"\n",
        "\n",
        "    # Preprocessor for global use\n",
        "    pre, num_cols, cat_cols = make_preprocessor(df)\n",
        "\n",
        "    # ===== 5) SEPARATE RESULTS BY CATEGORY × SELECTOR =====\n",
        "    all_blocks = run_all_blocks(df, TARGET_COL)\n",
        "    print(\"\\n=== Blocked Results (Group × Selector) ===\")\n",
        "    for b in all_blocks:\n",
        "        print(f\"\\n[{b['group']}] via {b['selector']}\")\n",
        "        print(\"Selected features:\", b[\"selected_features\"])\n",
        "        print(pd.DataFrame(b[\"results\"]).T.round(3))\n",
        "\n",
        "    # ===== 6) COURSE-WISE =====\n",
        "    if COURSE_COL in df.columns:\n",
        "        cw = course_wise_performance(df, TARGET_COL)\n",
        "        print(\"\\n=== Course-wise results (classical models) ===\")\n",
        "        print(cw.round(3).to_string(index=False))\n",
        "\n",
        "    # ===== 7) XAI (LIME/SHAP) on a representative model with top features (e.g., IG over ALL features) =====\n",
        "    usable_feats = keep_existing(MENTAL_FEATURES+SOCIAL_FEATURES+PERSONAL_FEATURES, df)\n",
        "    sc_all = feature_scores(df[usable_feats], df[TARGET_COL], usable_feats)\n",
        "    top10 = select_by_criterion(sc_all, \"info_gain\", top_k=10)\n",
        "    xai = run_xai_examples(df, TARGET_COL, top10, pre)\n",
        "    print(\"\\n=== XAI ===\")\n",
        "    if HAVE_SHAP: print(\"SHAP shapes:\", xai.get(\"shap_values_shape\"))\n",
        "    else: print(\"SHAP not available.\")\n",
        "    if HAVE_LIME: print(\"LIME top features (first instance):\", xai.get(\"lime_top\"))\n",
        "    else: print(\"LIME not available.\")\n",
        "\n",
        "    # ===== 8) Statistical tests =====\n",
        "    stats_df = statistical_tests(df, TARGET_COL, usable_feats)\n",
        "    print(\"\\n=== Statistical tests ===\")\n",
        "    print(stats_df.round(4).to_string(index=False))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}